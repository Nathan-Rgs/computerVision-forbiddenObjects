{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17204685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\workspace\\faculda_10_sem\\VisaoComputacionalProjeto\\computerVision-forbiddenObjects\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ab057e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kaggle kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ffdd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"samuelayman/backpack\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0022cd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Caminho do dataset: {path}\")\n",
    "print(\"\\nConte√∫do do diret√≥rio:\")\n",
    "for item in os.listdir(path):\n",
    "    item_path = os.path.join(path, item)\n",
    "    if os.path.isdir(item_path):\n",
    "        print(f\"{item}/\")\n",
    "    else:\n",
    "        print(f\"{item}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2744d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- CONFIGURA√á√ÉO ---\n",
    "# 1. Defina o caminho para a pasta que cont√©m os arquivos .txt de anota√ß√£o\n",
    "labels_dir = f\"{path}/backpack/labels\"  # Mude aqui se sua pasta tiver outro nome ou caminho\n",
    "\n",
    "# 2. Defina o ID da classe que voc√™ quer adicionar. Para \"backpack\", vamos usar 0.\n",
    "class_id_to_add = 1 #minha classe 0 vai ser as mochilas\n",
    "# --- FIM DA CONFIGURA√á√ÉO ---\n",
    "\n",
    "\n",
    "def add_class_to_annotations():\n",
    "    \"\"\"\n",
    "    Percorre todos os arquivos .txt no diret√≥rio especificado e\n",
    "    adiciona o ID da classe no in√≠cio de cada linha.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(labels_dir):\n",
    "        print(f\"Erro: O diret√≥rio '{labels_dir}' n√£o foi encontrado.\")\n",
    "        return\n",
    "\n",
    "    # Lista todos os arquivos no diret√≥rio\n",
    "    files = os.listdir(labels_dir)\n",
    "    txt_files = [f for f in files if f.endswith('.txt')]\n",
    "\n",
    "    if not txt_files:\n",
    "        print(f\"Nenhum arquivo .txt encontrado em '{labels_dir}'.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Encontrados {len(txt_files)} arquivos de anota√ß√£o. Iniciando o processo...\")\n",
    "    \n",
    "    processed_count = 0\n",
    "    for filename in txt_files:\n",
    "        filepath = os.path.join(labels_dir, filename)\n",
    "        \n",
    "        # L√™ todas as linhas do arquivo\n",
    "        with open(filepath, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # Verifica se o arquivo j√° foi processado para n√£o adicionar o ID duas vezes\n",
    "        if lines and lines[0].strip().startswith(str(class_id_to_add)):\n",
    "            # print(f\"Arquivo '{filename}' j√° parece estar no formato correto. Pulando.\")\n",
    "            continue\n",
    "            \n",
    "        new_lines = []\n",
    "        for line in lines:\n",
    "            # Remove espa√ßos em branco extras e verifica se a linha n√£o est√° vazia\n",
    "            if line.strip():\n",
    "                new_line = f\"{class_id_to_add} {line.strip()}\\n\"\n",
    "                new_lines.append(new_line)\n",
    "\n",
    "        # Escreve o novo conte√∫do de volta no arquivo\n",
    "        with open(filepath, 'w') as f:\n",
    "            f.writelines(new_lines)\n",
    "        \n",
    "        processed_count += 1\n",
    "\n",
    "    print(f\"\\nProcesso conclu√≠do!\")\n",
    "    print(f\"{processed_count} arquivos foram atualizados com o ID de classe '{class_id_to_add}'.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    add_class_to_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1baccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar bibliotecas necess√°rias\n",
    "#! pip install tensorflow opencv-python matplotlib scikit-learn pillow seaborn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f66daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir caminhos base para o resto do pipeline\n",
    "BASE_PATH = path\n",
    "IMAGE_DIR = os.path.join(BASE_PATH, 'backpack')\n",
    "LABEL_DIR = os.path.join(BASE_PATH, \"backpack/labels\")\n",
    "\n",
    "BASE_PATH, IMAGE_DIR, labels_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2d600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Criar a estrutura de diret√≥rios\n",
    "output_dir = os.path.join(BASE_PATH, \"dataset\")\n",
    "for split in ['train', 'val']:\n",
    "    os.makedirs(os.path.join(output_dir, split, 'images'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, split, 'labels'), exist_ok=True)\n",
    "\n",
    "# Listar todas as imagens\n",
    "image_files = [f for f in os.listdir(IMAGE_DIR) if f.endswith('.jpg')]\n",
    "\n",
    "print(len(image_files))\n",
    "\n",
    "train_files, val_files = train_test_split(image_files, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Total de imagens: {len(image_files)}\")\n",
    "print(f\"Imagens de treino: {len(train_files)}\")\n",
    "print(f\"Imagens de valida√ß√£o: {len(val_files)}\")\n",
    "\n",
    "# Fun√ß√£o para mover os arquivos\n",
    "def move_files(file_list, split_name):\n",
    "    for filename in file_list:\n",
    "        basename = os.path.splitext(filename)[0]\n",
    "        \n",
    "        # Mover imagem\n",
    "        shutil.copy(\n",
    "            os.path.join(IMAGE_DIR, f\"{basename}.jpg\"),\n",
    "            os.path.join(output_dir, split_name, 'images', f\"{basename}.jpg\")\n",
    "        )\n",
    "        # Mover anota√ß√£o\n",
    "        shutil.copy(\n",
    "            os.path.join(LABEL_DIR, f\"{basename}.txt\"),\n",
    "            os.path.join(output_dir, split_name, 'labels', f\"{basename}.txt\")\n",
    "        )\n",
    "\n",
    "# Mover os arquivos para suas respectivas pastas\n",
    "move_files(train_files, 'train')\n",
    "move_files(val_files, 'val')\n",
    "\n",
    "print(\"Dataset dividido e organizado com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b243977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterRCNN(tf.keras.Model):\n",
    "    def __init__(self, num_classes, num_anchors=9, pool_size=7, **kwargs):\n",
    "        \"\"\"\n",
    "        Construtor do modelo. Inicializa todos os componentes.\n",
    "        \"\"\"\n",
    "        super(FasterRCNN, self).__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Define o formato de entrada para o backbone\n",
    "        input_shape = [*IMAGE_SIZE, 3]\n",
    "        \n",
    "        # Instanciar todos os componentes\n",
    "        self.backbone = get_backbone(input_shape=input_shape)\n",
    "        self.rpn = get_rpn(num_anchors)\n",
    "        self.roi_align = RoiAlignLayer(pool_size)\n",
    "        self.detector = get_detector_head(pool_size, num_classes)\n",
    "        \n",
    "        # Fun√ß√µes de perda\n",
    "        self.rpn_cls_loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "        self.rpn_reg_loss_fn = tf.keras.losses.Huber()\n",
    "        self.det_cls_loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "        self.det_reg_loss_fn = tf.keras.losses.Huber()\n",
    "   \n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        Este √© o m√©todo que faltava. Define o forward pass.\n",
    "        \"\"\"\n",
    "        # 1. Passar a imagem pelo backbone para extrair features\n",
    "        feature_maps = self.backbone(inputs, training=training)\n",
    "        \n",
    "        # 2. Obter propostas da Region Proposal Network (RPN)\n",
    "        rpn_cls_output, rpn_reg_output = self.rpn(feature_maps)\n",
    "        \n",
    "        # --- L√ìGICA DE PROPOSTAS (MUITO SIMPLIFICADA) ---\n",
    "        num_proposals = 300\n",
    "        dummy_proposals = tf.random.uniform(shape=[tf.shape(inputs)[0], num_proposals, 4], dtype=tf.float32)\n",
    "        \n",
    "        # 3. Usar RoI Align para extrair features das propostas\n",
    "        roi_features = self.roi_align([feature_maps, dummy_proposals])\n",
    "        \n",
    "        # --- CORRE√á√ÉO AQUI ---\n",
    "        # A sa√≠da do roi_align √© (batch_size * num_rois, ...).\n",
    "        # Precisamos dar um reshape para (batch_size, num_rois, ...)\n",
    "        # para que seja compat√≠vel com as camadas TimeDistributed do detector.\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        # O √∫ltimo canal '512' vem do backbone VGG16 (block5_conv3)\n",
    "        reshaped_roi_features = tf.reshape(\n",
    "            roi_features, \n",
    "            [batch_size, num_proposals, self.roi_align.pool_size, self.roi_align.pool_size, 512]\n",
    "        )\n",
    "        \n",
    "        # 4. Passar as features reshaped pelo detector final\n",
    "        detector_cls_output, detector_reg_output = self.detector(reshaped_roi_features)\n",
    "        \n",
    "        return rpn_cls_output, rpn_reg_output, detector_cls_output, detector_reg_output\n",
    "    def train_step(self, data):\n",
    "        \"\"\"\n",
    "        Define um passo de treinamento customizado.\n",
    "        \"\"\"\n",
    "        # Desempacota os dados. Nosso pipeline de dados retorna (imagem, caixas, classes)\n",
    "        images, gt_boxes, gt_classes = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Obter todas as sa√≠das do modelo chamando o m√©todo 'call'\n",
    "            rpn_cls, rpn_reg, det_cls, det_reg = self(images, training=True)\n",
    "            \n",
    "            # --- L√ìGICA DE ATRIBUI√á√ÉO DE ALVOS E C√ÅLCULO DE PERDA (AINDA SIMPLIFICADA) ---\n",
    "            # Como explicado antes, a l√≥gica para criar os alvos (targets) a partir\n",
    "            # das caixas de verdade (gt_boxes) √© complexa. Usamos placeholders aqui\n",
    "            # para garantir que o c√≥digo rode sem erros.\n",
    "            \n",
    "            # Alvos falsos para a RPN\n",
    "            rpn_shape = tf.shape(rpn_cls)\n",
    "            dummy_rpn_cls_target = tf.random.uniform(rpn_shape, maxval=2, dtype=tf.int32)\n",
    "            dummy_rpn_reg_target = tf.zeros_like(rpn_reg)\n",
    "            \n",
    "            # Alvos falsos para o Detector\n",
    "            det_shape = tf.shape(det_cls)\n",
    "            dummy_det_cls_target = tf.random.uniform([det_shape[0], det_shape[1]], maxval=self.num_classes, dtype=tf.int32)\n",
    "            dummy_det_reg_target = tf.zeros_like(det_reg)\n",
    "            \n",
    "            # C√°lculo das perdas\n",
    "            rpn_cls_loss = self.rpn_cls_loss_fn(dummy_rpn_cls_target, rpn_cls)\n",
    "            rpn_reg_loss = self.rpn_reg_loss_fn(dummy_rpn_reg_target, rpn_reg)\n",
    "            det_cls_loss = self.det_cls_loss_fn(dummy_det_cls_target, det_cls)\n",
    "            det_reg_loss = self.det_reg_loss_fn(dummy_det_reg_target, det_reg)\n",
    "            \n",
    "            total_loss = rpn_cls_loss + rpn_reg_loss + det_cls_loss + det_reg_loss\n",
    "\n",
    "        # Calcular e aplicar os gradientes para atualizar os pesos do modelo\n",
    "        grads = tape.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        \n",
    "        return {\n",
    "            \"total_loss\": total_loss,\n",
    "            \"rpn_cls_loss\": rpn_cls_loss,\n",
    "            \"rpn_reg_loss\": rpn_reg_loss,\n",
    "            \"det_cls_loss\": det_cls_loss,\n",
    "            \"det_reg_loss\": det_reg_loss,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fd38c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "IMAGE_SIZE = (600, 600)\n",
    "BATCH_SIZE = 2 # Comece com um valor baixo devido ao uso de mem√≥ria\n",
    "\n",
    "def parse_yolo_annotation(label_path):\n",
    "    \"\"\"L√™ um arquivo de anota√ß√£o YOLO e a imagem correspondente.\"\"\"\n",
    "    label_path = tf.strings.strip(label_path)\n",
    "    \n",
    "    # Encontra o caminho da imagem a partir do caminho da anota√ß√£o\n",
    "    img_path = tf.strings.regex_replace(label_path, \"[/\\\\\\\\]labels[/\\\\\\\\]\", \"/images/\")\n",
    "    img_path = tf.strings.regex_replace(img_path, \".txt$\", \".jpg\")\n",
    "    \n",
    "    # Carrega e decodifica a imagem\n",
    "    image = tf.io.read_file(img_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    original_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
    "    \n",
    "    # Redimensiona a imagem\n",
    "    image = tf.image.resize(image, IMAGE_SIZE)\n",
    "    \n",
    "    # Carrega as anota√ß√µes\n",
    "    label_content = tf.io.read_file(label_path)\n",
    "    lines = tf.strings.split(label_content, '\\n')\n",
    "    \n",
    "    # Usar TensorArray para boxes e classes\n",
    "    num_lines = tf.shape(lines)[0]\n",
    "    boxes_ta = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\n",
    "    classes_ta = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\n",
    "    \n",
    "    def cond(i, boxes_ta, classes_ta):\n",
    "        return i < num_lines\n",
    "\n",
    "    def body(i, boxes_ta, classes_ta):\n",
    "        line = lines[i]\n",
    "        # Ignorar linhas vazias\n",
    "        def process_line():\n",
    "            parts = tf.strings.split(line, ' ')\n",
    "            class_id = tf.strings.to_number(parts[0], out_type=tf.float32)\n",
    "            x_center = tf.strings.to_number(parts[1], out_type=tf.float32)\n",
    "            y_center = tf.strings.to_number(parts[2], out_type=tf.float32)\n",
    "            width = tf.strings.to_number(parts[3], out_type=tf.float32)\n",
    "            height = tf.strings.to_number(parts[4], out_type=tf.float32)\n",
    "            y1 = y_center - (height / 2.0)\n",
    "            x1 = x_center - (width / 2.0)\n",
    "            y2 = y_center + (height / 2.0)\n",
    "            x2 = x_center + (width / 2.0)\n",
    "            boxes_ta_new = boxes_ta.write(boxes_ta.size(), [y1, x1, y2, x2])\n",
    "            classes_ta_new = classes_ta.write(classes_ta.size(), class_id)\n",
    "            return boxes_ta_new, classes_ta_new\n",
    "\n",
    "        def skip_line():\n",
    "            return boxes_ta, classes_ta\n",
    "\n",
    "        boxes_ta, classes_ta = tf.cond(\n",
    "            tf.strings.length(line) > 0,\n",
    "            process_line,\n",
    "            skip_line\n",
    "        )\n",
    "        return i + 1, boxes_ta, classes_ta\n",
    "\n",
    "    i = tf.constant(0)\n",
    "    _, boxes_ta, classes_ta = tf.while_loop(\n",
    "        cond, body, [i, boxes_ta, classes_ta]\n",
    "    )\n",
    "\n",
    "    boxes = boxes_ta.stack()\n",
    "    classes = classes_ta.stack()\n",
    "    \n",
    "    return image, boxes, classes\n",
    "\n",
    "def create_dataset(data_dir):\n",
    "    \"\"\"Cria um tf.data.Dataset a partir de um diret√≥rio de anota√ß√µes.\"\"\"\n",
    "    label_files = tf.data.Dataset.list_files(os.path.join(data_dir, \"labels\", \"*.txt\"))\n",
    "    \n",
    "    dataset = label_files.map(parse_yolo_annotation, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # O collate_fn √© necess√°rio porque as imagens t√™m n√∫meros diferentes de caixas.\n",
    "    # O `padded_batch` garante que todos os tensores em um lote tenham o mesmo tamanho.\n",
    "    dataset = dataset.padded_batch(\n",
    "        BATCH_SIZE,\n",
    "        padding_values=(tf.constant(0.0), tf.constant(0.0), tf.constant(-1.0)),\n",
    "        padded_shapes=([*IMAGE_SIZE, 3], [None, 4], [None]),\n",
    "        drop_remainder=True\n",
    "    )\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Criar os datasets de treino e valida√ß√£o\n",
    "train_dir = os.path.join(output_dir, \"train\")\n",
    "val_dir = os.path.join(output_dir, \"val\")\n",
    "\n",
    "train_dataset = create_dataset(train_dir)\n",
    "val_dataset = create_dataset(val_dir)\n",
    "\n",
    "print(\"‚úÖ Datasets criados com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbe6ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 1 # Apenas 'backpack'\n",
    "\n",
    "# Instanciar e compilar o modelo\n",
    "model = FasterRCNN(num_classes=NUM_CLASSES)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "# Iniciar o treinamento\n",
    "print(\"üöÄ Iniciando o treinamento...\")\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=10, # Comece com poucas √©pocas\n",
    "    validation_data=val_dataset,\n",
    "    verbose=1\n",
    ")\n",
    "print(\"‚úÖ Treinamento conclu√≠do!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c18dd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def predict(model, image_path, confidence_threshold=0.5):\n",
    "    # Carregar e pr√©-processar a imagem\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image_resized = tf.image.resize(image, IMAGE_SIZE)\n",
    "    image_tensor = tf.expand_dims(image_resized, axis=0)\n",
    "    \n",
    "    # Fazer a predi√ß√£o\n",
    "    _, _, det_cls, det_reg = model(image_tensor, training=False)\n",
    "\n",
    "    # --- L√ìGICA DE P√ìS-PROCESSAMENTO (SIMPLIFICADA) ---\n",
    "    # Em uma implementa√ß√£o real, voc√™ precisa:\n",
    "    # 1. Decodificar as propostas da RPN e aplicar NMS.\n",
    "    # 2. Usar as propostas filtradas no detector.\n",
    "    # 3. Decodificar as caixas finais do detector.\n",
    "    # 4. Aplicar NMS novamente nas caixas finais.\n",
    "    # -------------------------------------------------\n",
    "    \n",
    "    # Para este exemplo, vamos apenas olhar a sa√≠da do detector\n",
    "    scores = tf.reduce_max(det_cls[0], axis=1)\n",
    "    \n",
    "    # Filtra as detec√ß√µes com base na confian√ßa\n",
    "    selected_indices = tf.where(scores > confidence_threshold)\n",
    "    \n",
    "    # Pega as melhores caixas (ainda n√£o foram refinadas pela regress√£o, isso √© outra etapa)\n",
    "    # A l√≥gica de decodifica√ß√£o das propostas dummy n√£o foi implementada, ent√£o as caixas n√£o ser√£o precisas\n",
    "    \n",
    "    print(f\"Encontradas {len(selected_indices)} detec√ß√µes acima do limiar de confian√ßa.\")\n",
    "    \n",
    "    # Desenhar as caixas (aqui seria necess√°rio decodificar as caixas)\n",
    "    # Por simplicidade, vamos pular o desenho, pois as coordenadas n√£o s√£o significativas sem a l√≥gica completa.\n",
    "    \n",
    "    final_image = np.array(image)\n",
    "    # ... c√≥digo para desenhar as caixas finais em `final_image` usando OpenCV ...\n",
    "    \n",
    "    return final_image\n",
    "\n",
    "\n",
    "# Exemplo de uso\n",
    "# Supondo que o modelo foi treinado de verdade\n",
    "# result_image = predict(model, 'caminho/para/imagem_teste.jpg')\n",
    "# cv2.imwrite('resultado.jpg', result_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
